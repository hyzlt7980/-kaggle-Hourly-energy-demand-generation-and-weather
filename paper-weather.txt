https://www.bgc-jena.mpg.de/wetter/

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from sklearn.preprocessing import StandardScaler

# ==========================================
# 1. Âü∫Á°ÄÁªÑ‰ª∂ (RevIN & Transpose)
# ==========================================
class Transpose(nn.Module):
    def __init__(self, dim0, dim1):
        super().__init__()
        self.dim0, self.dim1 = dim0, dim1
    def forward(self, x): return x.transpose(self.dim0, self.dim1)

class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.affine_weight = nn.Parameter(torch.ones(num_features))
        self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode: str):
        if mode == 'norm':
            self.mean = x.mean(1, keepdim=True).detach()
            self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
            return (x - self.mean) / self.stdev * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            return (x - self.affine_bias) / self.affine_weight * self.stdev + self.mean

# ==========================================
# 2. ÂºÇÊûÑ‰∏ìÂÆ∂ÂÆö‰πâ (‰øÆÂ§ç Inception Áª¥Â∫¶Ëá≥ 256)
# ==========================================
class SpectralExpert(nn.Module):
    def __init__(self, seq_len, d_model):
        super().__init__()
        self.fc = nn.Linear(seq_len // 2 + 1, d_model)
    def forward(self, x): return self.fc(torch.abs(torch.fft.rfft(x, dim=-1)))

class InceptionExpert(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        # 4Ë∑ØÂç∑ÁßØÁ°Æ‰øùËæìÂá∫Áª¥Â∫¶ = d_model (256)
        self.c1 = nn.Conv1d(1, d_model//4, 3, padding=1)
        self.c2 = nn.Conv1d(1, d_model//4, 7, padding=3)
        self.c3 = nn.Conv1d(1, d_model//4, 11, padding=5)
        self.c4 = nn.Conv1d(1, d_model//4, 15, padding=7)
        self.pool = nn.AdaptiveAvgPool1d(1)
    def forward(self, x):
        out = torch.cat([self.c1(x.unsqueeze(1)), self.c2(x.unsqueeze(1)), 
                         self.c3(x.unsqueeze(1)), self.c4(x.unsqueeze(1))], dim=1)
        return self.pool(F.gelu(out)).flatten(1)

class SwinBlock1D(nn.Module):
    def __init__(self, dim, seq_len):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, 8, batch_first=True)
    def forward(self, x):
        h = x; x = self.norm(x); x, _ = self.attn(x, x, x)
        return x + h

class IntraColumnPentaExperts(nn.Module):
    def __init__(self, seq_len=96, d_model=256, path_drop=0.05):
        super().__init__()
        self.path_drop = path_drop
        self.e1, self.e2 = SpectralExpert(seq_len, d_model), InceptionExpert(d_model)
        self.e3 = nn.Sequential(nn.Linear(1, d_model), SwinBlock1D(d_model, seq_len), Transpose(1, 2), nn.AdaptiveAvgPool1d(1), nn.Flatten())
        self.e4 = nn.Sequential(nn.Linear(seq_len - 1, d_model), nn.LayerNorm(d_model))
        self.e5 = nn.Sequential(nn.Linear(seq_len, d_model), nn.GELU(), nn.Linear(d_model, d_model))
        self.gate = nn.Sequential(nn.Linear(d_model * 5, 512), nn.GELU(), nn.Linear(512, 5))
        self.temp = 1.5

    def forward(self, x_strip):
        f1, f2 = self.e1(x_strip), self.e2(x_strip)
        f3 = self.e3(x_strip.unsqueeze(-1))
        f4 = self.e4(x_strip[:, 1:] - x_strip[:, :-1])
        f5 = self.e5(x_strip)
        experts = [f1, f2, f3, f4, f5]
        if self.training:
            for i in range(5):
                if torch.rand(1) < self.path_drop: experts[i] = experts[i] * 0.0 
        w = F.softmax(self.gate(torch.cat(experts, dim=-1)) / self.temp, dim=-1)
        return sum(w[:, i:i+1] * experts[i] for i in range(5)), w

class XManifoldUltra(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=21, d_model=256):
        super().__init__()
        self.revin = RevIN(num_vars)
        self.experts = IntraColumnPentaExperts(seq_len, d_model)
        layer = nn.TransformerEncoderLayer(d_model, 8, d_model*4, 0.1, batch_first=True, norm_first=True)
        self.manifold = nn.TransformerEncoder(layer, num_layers=3)
        self.projection = nn.Linear(d_model, pred_len)

    def forward(self, x, denorm=False):
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        x_flat = x.permute(0, 2, 1).reshape(B * N, L)
        tokens, weights = self.experts(x_flat)
        tokens = tokens.view(B, N, -1)
        tokens = self.manifold(tokens)
        out = self.projection(tokens).permute(0, 2, 1)
        if denorm: out = self.revin(out, 'denorm')
        return out, weights

# ==========================================
# 3. Êï∞ÊçÆÈõÜ (‰øÆÊ≠£ Scaler ÈÄªËæë)
# ==========================================
class WeatherDataset(Dataset):
    def __init__(self, path, flag='train', seq_len=96, pred_len=96, scaler=None):
        df = pd.read_csv(path)
        data = df.iloc[:, 1:].values
        n = len(data); tr, vl = int(n*0.7), int(n*0.8)
        
        if flag == 'train':
            self.scaler = StandardScaler()
            self.scaler.fit(data[:tr])
            self.data = self.scaler.transform(data[:tr])
        elif flag == 'val':
            self.scaler = scaler
            self.data = self.scaler.transform(data[tr-seq_len : vl])
        else: # test
            self.scaler = scaler
            self.data = self.scaler.transform(data[vl-seq_len:])
            
        self.seq_len, self.pred_len = seq_len, pred_len

    def __getitem__(self, i):
        return torch.tensor(self.data[i:i+self.seq_len], dtype=torch.float32), \
               torch.tensor(self.data[i+self.seq_len:i+self.seq_len+self.pred_len], dtype=torch.float32)
    def __len__(self): return len(self.data) - self.seq_len - self.pred_len + 1

# ==========================================
# 4. ‰∏ªÁ®ãÂ∫è (DDP + Áªü‰∏ÄÊµãËØïÈÄªËæë)
# ==========================================
def main():
    dist.init_process_group("nccl"); local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    
    path = './dataset/weather.csv' if os.path.exists('./dataset/weather.csv') else 'weather.csv'
    
    # üö® Ê≠•È™§ 1: ÂÖàÂàùÂßãÂåñËÆ≠ÁªÉÈõÜËé∑ÂèñÂÖ®Â±Ä Scaler
    ds_train = WeatherDataset(path, flag='train')
    global_scaler = ds_train.scaler 
    
    sampler = DistributedSampler(ds_train)
    train_loader = DataLoader(ds_train, batch_size=128, sampler=sampler, num_workers=8, pin_memory=True)

    model = DDP(XManifoldUltra().cuda(), device_ids=[local_rank])
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=20)
    criterion = nn.MSELoss()

    expert_names = ['Spec', 'Incp', 'Swin', 'Diff', 'Glob']

    if local_rank == 0: print("üöÄ XManifold Ultra Pro [Unified Scaler] ÂêØÂä®...")

    for epoch in range(20):
        model.train(); sampler.set_epoch(epoch)
        for i, (bx, by) in enumerate(train_loader):
            bx, by = bx.cuda(), by.cuda()
            optimizer.zero_grad()
            output, weights = model(bx)
            loss = criterion(output, by); loss.backward()
            optimizer.step(); scheduler.step()
            
            if i % 100 == 0 and local_rank == 0:
                w_avg = weights.mean(dim=0).detach().cpu().numpy()
                w_str = " | ".join([f"{n}:{w:.3f}" for n, w in zip(expert_names, w_avg)])
                print(f"Epoch {epoch+1:02d} | Iter {i} | Loss: {loss.item():.4f} | Weights: [{w_str}]")

    # üö® Ê≠•È™§ 2: ÊúÄÁªàÊµãËØï (‰ΩøÁî® Rank 0 ËøêË°åÔºå‰º†ÂÖ•ÂÖ®Â±Ä Scaler)
    if local_rank == 0:
        print("\n" + "="*30 + "\nüèÜ ËÆ≠ÁªÉÁªìÊùüÔºåËøêË°å Final Test (Eval Mode)...")
        ds_test = WeatherDataset(path, flag='test', scaler=global_scaler)
        test_loader = DataLoader(ds_test, batch_size=128, shuffle=False)
        model.eval() 
        total_mse = 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.cuda(), by.cuda()
                pred, _ = model(bx, denorm=False)
                total_mse += F.mse_loss(pred, by).item()
        
        print(f"‚úÖ Final Test MSE (Normalized): {total_mse/len(test_loader):.6f}")
        print("="*30)

    dist.destroy_process_group()

if __name__ == "__main__": main()
