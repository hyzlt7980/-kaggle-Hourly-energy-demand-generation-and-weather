import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. åŸºç¡€ç»„ä»¶
# ==========================================
class Transpose(nn.Module):
    def __init__(self, dim0, dim1):
        super().__init__()
        self.dim0, self.dim1 = dim0, dim1
    def forward(self, x): return x.transpose(self.dim0, self.dim1)

class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps, self.affine = eps, affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode: str):
        if mode == 'norm':
            self.mean = x.mean(1, keepdim=True).detach()
            self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / self.affine_weight
            x = x * self.stdev + self.mean
        return x

# ==========================================
# 1. å¼‚æ„äº”ä¸“å®¶ç»„ä»¶ (XManifold æ ¸å¿ƒ)
# ==========================================
class SpectralExpert(nn.Module):
    def __init__(self, seq_len, d_model):
        super().__init__()
        self.fc = nn.Linear(seq_len // 2 + 1, d_model)
    def forward(self, x):
        x_fft = torch.fft.rfft(x, dim=-1)
        return self.fc(torch.abs(x_fft))

class InceptionExpert(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.c1 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=1)
        self.c2 = nn.Conv1d(1, d_model//4, kernel_size=7, padding=3)
        self.c3 = nn.Conv1d(1, d_model//4, kernel_size=11, padding=5)
        self.c4 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=2, dilation=2)
        self.pool = nn.AdaptiveAvgPool1d(1)
    def forward(self, x):
        x = x.unsqueeze(1)
        out = torch.cat([self.c1(x), self.c2(x), self.c3(x), self.c4(x)], dim=1)
        return self.pool(F.gelu(out)).flatten(1)

class SwinWindowAttention1D(nn.Module):
    def __init__(self, dim, window_size, shift_size, num_heads=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size, self.num_heads = dim, window_size, shift_size, num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.qkv = nn.Linear(dim, dim * 3); self.proj = nn.Linear(dim, dim)
        self.rel_pos_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1), num_heads))
        nn.init.trunc_normal_(self.rel_pos_bias_table, std=.02)
    def forward(self, x, mask=None):
        B_N, num_win, ws, D = x.shape
        qkv = self.qkv(x).reshape(B_N, num_win, ws, 3, self.num_heads, D // self.num_heads).permute(3, 0, 1, 4, 2, 5)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        coords = torch.arange(ws); rel_index = (coords[None, :] - coords[:, None] + ws - 1).to(x.device)
        rel_pos_bias = self.rel_pos_bias_table[rel_index.view(-1)].view(ws, ws, -1).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)
        attn = attn + rel_pos_bias
        if mask is not None: attn = attn + mask.unsqueeze(1).unsqueeze(0)
        attn = F.softmax(attn, dim=-1)
        return self.proj((attn @ v).transpose(2, 3).reshape(B_N, num_win, ws, D))

class SwinBlock1D(nn.Module):
    def __init__(self, dim, seq_len, window_size=8, shift_size=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size = dim, window_size, shift_size
        self.norm1 = nn.LayerNorm(dim); self.attn = SwinWindowAttention1D(dim, window_size, 0)
        self.norm2 = nn.LayerNorm(dim); self.shift_attn = SwinWindowAttention1D(dim, window_size, shift_size)
        if self.shift_size > 0:
            img_mask = torch.zeros((1, seq_len, 1))
            for i, s in enumerate((slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))): img_mask[:, s, :] = i
            m_win = img_mask.view(1, seq_len // window_size, window_size, 1).reshape(-1, window_size)
            attn_mask = m_win.unsqueeze(1) - m_win.unsqueeze(2)
            self.register_buffer("attn_mask", attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0))
        else: self.attn_mask = None
    def forward(self, x):
        B_N, L, D = x.shape
        h = x; x = self.norm1(x).view(B_N, L // self.window_size, self.window_size, D)
        x = self.attn(x).view(B_N, L, D) + h
        h = x; x = self.norm2(x)
        if self.shift_size > 0: x = torch.roll(x, shifts=-self.shift_size, dims=1)
        x = x.view(B_N, L // self.window_size, self.window_size, D)
        x = self.shift_attn(x, mask=self.attn_mask).view(B_N, L, D)
        if self.shift_size > 0: x = torch.roll(x, shifts=self.shift_size, dims=1)
        return x + h

class IntraColumnPentaExperts(nn.Module):
    def __init__(self, seq_len=96, d_model=256, path_drop=0.35):
        super().__init__()
        self.path_drop = path_drop
        self.exp1_spectral = SpectralExpert(seq_len, d_model)
        self.exp2_inception = InceptionExpert(d_model)
        self.exp3_swin = nn.Sequential(nn.Linear(1, d_model), SwinBlock1D(d_model, seq_len), Transpose(1, 2), nn.AdaptiveAvgPool1d(1), nn.Flatten())
        self.exp4_diff = nn.Sequential(nn.Linear(seq_len - 1, d_model), nn.LayerNorm(d_model))
        self.exp5_global = nn.Sequential(nn.Linear(seq_len, d_model), nn.ReLU(), nn.Linear(d_model, d_model))
        self.gate = nn.Sequential(nn.Linear(d_model * 5, 256), nn.GELU(), nn.Linear(256, 5))
        self.temp = 2.0 
    def forward(self, x_strip):
        f1, f2 = self.exp1_spectral(x_strip), self.exp2_inception(x_strip)
        f3 = self.exp3_swin(x_strip.unsqueeze(-1))
        f4 = self.exp4_diff(x_strip[:, 1:] - x_strip[:, :-1])
        f5 = self.exp5_global(x_strip)
        experts = [f1, f2, f3, f4, f5]
        
        if self.training:
            for i in range(5):
                # ğŸ”‘ ä¿®å¤ç‚¹ï¼šä¸ºäº†ä¿è¯ DDP æ¢¯åº¦å›¾ä¸æ–­è£‚ï¼Œä¸è¦ç”¨ zeros_likeï¼Œè€Œæ˜¯ä¹˜ä»¥ 0.0
                if torch.rand(1) < self.path_drop: 
                    experts[i] = experts[i] * 0.0 
                    
        w = F.softmax(self.gate(torch.cat(experts, dim=-1)) / self.temp, dim=-1)
        return sum(w[:, i:i+1] * experts[i] for i in range(5))

# ==========================================
# 2. XManifoldUltra (96->96 å›å½’æ”¹é€ )
# ==========================================
class XManifoldUltra(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=14, d_model=256, noise_std=0.01):
        super().__init__()
        self.noise_std, self.revin = noise_std, RevIN(num_vars)
        self.experts = IntraColumnPentaExperts(seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model, 8, d_model*4, 0.2, batch_first=True, norm_first=True)
        self.game_room = nn.TransformerEncoder(encoder_layer, num_layers=6) 
        
        # ğŸ”‘ ä¿®å¤ 1ï¼šå°†å›å½’å¤´çš„ Dropout ä» 0.45 æš´é™åˆ° 0.1
        self.head = nn.Sequential(
            nn.Linear(num_vars * d_model, 512), 
            nn.GELU(), 
            nn.Dropout(0.1), 
            nn.Linear(512, pred_len)
        )
        
    def forward(self, x):
        # 1. å±€éƒ¨å½’ä¸€åŒ–
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        tokens = self.experts(x.permute(0, 2, 1).reshape(B * N, L)).view(B, N, -1)
        if self.training: tokens = tokens + torch.randn_like(tokens) * self.noise_std
        
        # 2. é¢„æµ‹è¾“å‡º [Batch, 96]
        out = self.head(self.game_room(tokens).reshape(B, -1))
        
        # ğŸ”‘ ä¿®å¤ 2ï¼šé’ˆå¯¹ç¬¬ 0 åˆ—(ç”µä»·)è¿›è¡Œé€†å½’ä¸€åŒ– (Denorm)
        out = out.unsqueeze(-1) # å˜æˆ [Batch, 96, 1]
        if self.revin.affine:
            out = (out - self.revin.affine_bias[0]) / self.revin.affine_weight[0]
        # ä½¿ç”¨ç¬¬ 0 åˆ—çš„ std å’Œ mean è¿˜åŸå°ºåº¦
        out = out * self.revin.stdev[:, :, 0:1] + self.revin.mean[:, :, 0:1]
        
        return out.squeeze(-1) # æ¢å¤æˆ [Batch, 96]

# ==========================================
# 3. å•è¡¨è¯»å–ç‰ˆ Dataset (è¯»å– spain_energy.csv)
# ==========================================
class SpainDataset(Dataset):
    def __init__(self, csv_path, flag='train', seq_len=96, pred_len=96, scaler=None):
        df = pd.read_csv(csv_path)
        
        # ä¸¥æ ¼æŒ‰ç…§åˆå¹¶åçš„åˆ—åæå– (12ä¸ªåŸºç¡€ç‰¹å¾)
        cols = ['price actual', 'total load actual', 'generation wind onshore', 
                'generation solar', 'generation fossil gas', 'generation nuclear', 
                'generation fossil hard coal', 'temp', 'pressure', 'humidity', 
                'wind_speed', 'clouds_all']
        
        df_feat = df[cols].copy()
        
        # åŠ ä¸Šæ—¶é—´ç‰¹å¾ (2ä¸ª) -> å…±è®¡ 14 ä¸ªç‰¹å¾
        df['date'] = pd.to_datetime(df['date'])
        hour = df['date'].dt.hour
        df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
        n = len(df_feat)
        tr, vl = int(n * 0.8), int(n * 0.9)
        
        if flag == 'train':
            feat_raw = df_feat.iloc[:tr].values
            self.scaler = StandardScaler().fit(feat_raw)
        elif flag == 'val':
            feat_raw = df_feat.iloc[tr-seq_len:vl].values
            self.scaler = scaler
        else:
            feat_raw = df_feat.iloc[vl-seq_len:].values
            self.scaler = scaler
            
        self.feat = self.scaler.transform(feat_raw)
        self.seq_len = seq_len
        self.pred_len = pred_len

    def __len__(self): 
        # ç•™è¶³é¢„æµ‹çª—å£ï¼Œé˜²æ­¢è¶Šç•Œ
        return len(self.feat) - self.seq_len - self.pred_len + 1
        
    def __getitem__(self, i): 
        x = self.feat[i : i+self.seq_len]
        # æå–ç›®æ ‡ï¼šæœªæ¥ 96 æ­¥çš„ 'price actual' (ç¬¬ 0 åˆ—)
        y = self.feat[i+self.seq_len : i+self.seq_len+self.pred_len, 0]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# ==========================================
# 4. ä¸»è®­ç»ƒæµ (DDP ä¿®å¤ & å›å½’æŒ‡æ ‡) 
# ==========================================
def main():
    dist.init_process_group("nccl")
    rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    # å‡è®¾ spain_energy.csv åœ¨è¿è¡Œç›®å½•ä¸‹
    csv_file = 'spain_energy.csv'
    train_ds = SpainDataset(csv_file, 'train')
    val_ds = SpainDataset(csv_file, 'val', scaler=train_ds.scaler)
    test_ds = SpainDataset(csv_file, 'test', scaler=train_ds.scaler)
    
    # é‰´äº XManifold æä¸ºåºå¤§ï¼Œbatch_size è®¾ä¸º 32
    train_loader = DataLoader(train_ds, 32, sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, 32, sampler=DistributedSampler(val_ds, shuffle=False))
    test_loader = DataLoader(test_ds, 32, sampler=DistributedSampler(test_ds, shuffle=False))

    model = DDP(XManifoldUltra(num_vars=14, seq_len=96, pred_len=96).to(device), 
                device_ids=[local_rank], find_unused_parameters=False)
                
    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)
    scaler = GradScaler()
    
    # å›å½’ä»»åŠ¡æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    mae_criterion = nn.L1Loss()

    best_val_loss, best_path = float('inf'), "x_best_96.pth"
    if rank == 0: print("ğŸš€ XManifold Ultra (96->96 å›å½’å¯¹å†³ç‰ˆ) å¯åŠ¨...")

    for epoch in range(20):
        model.train()
        train_loader.sampler.set_epoch(epoch)
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            with autocast():
                loss = criterion(model(bx), by)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        scheduler.step()

        # éªŒè¯é›†è¯„ä¼°
        model.eval()
        local_val_loss, local_total = 0.0, 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                pred = model(bx)
                local_val_loss += criterion(pred, by).item() * by.size(0)
                local_total += by.size(0)
                
        stats = torch.tensor([local_val_loss, local_total], device=device)
        dist.all_reduce(stats, op=dist.ReduceOp.SUM) 
        
        if rank == 0:
            val_loss = stats[0].item() / stats[1].item()
            if val_loss < best_val_loss: 
                best_val_loss = val_loss
                torch.save(model.module.state_dict(), best_path)
                print(f"Epoch {epoch+1:02d} Val MSE Loss: {val_loss:.4f} ğŸŒŸ (New Best)")
            else:
                print(f"Epoch {epoch+1:02d} Val MSE Loss: {val_loss:.4f} | Best remains: {best_val_loss:.4f}")

    dist.barrier()
    
    # ä¿®å¤ DDP Bug: è®©æ‰€æœ‰è¿›ç¨‹åŒæ­¥åŠ è½½ Best Weights
    model.module.load_state_dict(torch.load(best_path, map_location=device))
    if rank == 0:
        print(f"\nğŸ¯ è®­ç»ƒç»“æŸï¼Œæœ€ä¼˜æƒé‡å·²åŒæ­¥ï¼Œå¼€å§‹å…¨é‡æµ‹è¯•...")
        
    model.eval()
    local_test_mse, local_test_mae, local_total = 0.0, 0.0, 0
    with torch.no_grad():
        for bx, by in test_loader:
            bx, by = bx.to(device), by.to(device)
            pred = model(bx)
            local_test_mse += criterion(pred, by).item() * by.size(0)
            local_test_mae += mae_criterion(pred, by).item() * by.size(0)
            local_total += by.size(0)
            
    test_stats = torch.tensor([local_test_mse, local_test_mae, local_total], device=device)
    dist.all_reduce(test_stats, op=dist.ReduceOp.SUM)
    
    if rank == 0:
        final_mse = test_stats[0].item() / test_stats[2].item()
        final_mae = test_stats[1].item() / test_stats[2].item()
        print(f"==================================================")
        print(f"ğŸŒŸ Final XManifold Ultra Test MSE : {final_mse:.4f}")
        print(f"ğŸŒŸ Final XManifold Ultra Test MAE : {final_mae:.4f}")
        print(f"==================================================")
        
    dist.destroy_process_group()

if __name__ == "__main__": main()

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. åŸºç¡€ç»„ä»¶
# ==========================================
class Transpose(nn.Module):
    def __init__(self, dim0, dim1):
        super().__init__()
        self.dim0, self.dim1 = dim0, dim1
    def forward(self, x): return x.transpose(self.dim0, self.dim1)

class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps, self.affine = eps, affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode: str):
        if mode == 'norm':
            self.mean = x.mean(1, keepdim=True).detach()
            self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / self.affine_weight
            x = x * self.stdev + self.mean
        return x

# ==========================================
# 1. å¼‚æ„äº”ä¸“å®¶ç»„ä»¶ (XManifold æ ¸å¿ƒ)
# ==========================================
class SpectralExpert(nn.Module):
    def __init__(self, seq_len, d_model):
        super().__init__()
        self.fc = nn.Linear(seq_len // 2 + 1, d_model)
    def forward(self, x):
        x_fft = torch.fft.rfft(x, dim=-1)
        return self.fc(torch.abs(x_fft))

class InceptionExpert(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.c1 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=1)
        self.c2 = nn.Conv1d(1, d_model//4, kernel_size=7, padding=3)
        self.c3 = nn.Conv1d(1, d_model//4, kernel_size=11, padding=5)
        self.c4 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=2, dilation=2)
        self.pool = nn.AdaptiveAvgPool1d(1)
    def forward(self, x):
        x = x.unsqueeze(1)
        out = torch.cat([self.c1(x), self.c2(x), self.c3(x), self.c4(x)], dim=1)
        return self.pool(F.gelu(out)).flatten(1)

class SwinWindowAttention1D(nn.Module):
    def __init__(self, dim, window_size, shift_size, num_heads=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size, self.num_heads = dim, window_size, shift_size, num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.qkv = nn.Linear(dim, dim * 3); self.proj = nn.Linear(dim, dim)
        self.rel_pos_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1), num_heads))
        nn.init.trunc_normal_(self.rel_pos_bias_table, std=.02)
    def forward(self, x, mask=None):
        B_N, num_win, ws, D = x.shape
        qkv = self.qkv(x).reshape(B_N, num_win, ws, 3, self.num_heads, D // self.num_heads).permute(3, 0, 1, 4, 2, 5)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        coords = torch.arange(ws); rel_index = (coords[None, :] - coords[:, None] + ws - 1).to(x.device)
        rel_pos_bias = self.rel_pos_bias_table[rel_index.view(-1)].view(ws, ws, -1).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)
        attn = attn + rel_pos_bias
        if mask is not None: attn = attn + mask.unsqueeze(1).unsqueeze(0)
        attn = F.softmax(attn, dim=-1)
        return self.proj((attn @ v).transpose(2, 3).reshape(B_N, num_win, ws, D))

class SwinBlock1D(nn.Module):
    def __init__(self, dim, seq_len, window_size=8, shift_size=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size = dim, window_size, shift_size
        self.norm1 = nn.LayerNorm(dim); self.attn = SwinWindowAttention1D(dim, window_size, 0)
        self.norm2 = nn.LayerNorm(dim); self.shift_attn = SwinWindowAttention1D(dim, window_size, shift_size)
        if self.shift_size > 0:
            img_mask = torch.zeros((1, seq_len, 1))
            for i, s in enumerate((slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))): img_mask[:, s, :] = i
            m_win = img_mask.view(1, seq_len // window_size, window_size, 1).reshape(-1, window_size)
            attn_mask = m_win.unsqueeze(1) - m_win.unsqueeze(2)
            self.register_buffer("attn_mask", attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0))
        else: self.attn_mask = None
    def forward(self, x):
        B_N, L, D = x.shape
        h = x; x = self.norm1(x).view(B_N, L // self.window_size, self.window_size, D)
        x = self.attn(x).view(B_N, L, D) + h
        h = x; x = self.norm2(x)
        if self.shift_size > 0: x = torch.roll(x, shifts=-self.shift_size, dims=1)
        x = x.view(B_N, L // self.window_size, self.window_size, D)
        x = self.shift_attn(x, mask=self.attn_mask).view(B_N, L, D)
        if self.shift_size > 0: x = torch.roll(x, shifts=self.shift_size, dims=1)
        return x + h

class IntraColumnPentaExperts(nn.Module):
    def __init__(self, seq_len=96, d_model=256, path_drop=0.35):
        super().__init__()
        self.path_drop = path_drop
        self.exp1_spectral = SpectralExpert(seq_len, d_model)
        self.exp2_inception = InceptionExpert(d_model)
        self.exp3_swin = nn.Sequential(nn.Linear(1, d_model), SwinBlock1D(d_model, seq_len), Transpose(1, 2), nn.AdaptiveAvgPool1d(1), nn.Flatten())
        self.exp4_diff = nn.Sequential(nn.Linear(seq_len - 1, d_model), nn.LayerNorm(d_model))
        self.exp5_global = nn.Sequential(nn.Linear(seq_len, d_model), nn.ReLU(), nn.Linear(d_model, d_model))
        self.gate = nn.Sequential(nn.Linear(d_model * 5, 256), nn.GELU(), nn.Linear(256, 5))
        self.temp = 2.0 
    def forward(self, x_strip):
        f1, f2 = self.exp1_spectral(x_strip), self.exp2_inception(x_strip)
        f3 = self.exp3_swin(x_strip.unsqueeze(-1))
        f4 = self.exp4_diff(x_strip[:, 1:] - x_strip[:, :-1])
        f5 = self.exp5_global(x_strip)
        experts = [f1, f2, f3, f4, f5]
        
        if self.training:
            for i in range(5):
                # ğŸ”‘ ä¿®å¤ç‚¹ï¼šä¸ºäº†ä¿è¯ DDP æ¢¯åº¦å›¾ä¸æ–­è£‚ï¼Œä¸è¦ç”¨ zeros_likeï¼Œè€Œæ˜¯ä¹˜ä»¥ 0.0
                if torch.rand(1) < self.path_drop: 
                    experts[i] = experts[i] * 0.0 
                    
        w = F.softmax(self.gate(torch.cat(experts, dim=-1)) / self.temp, dim=-1)
        return sum(w[:, i:i+1] * experts[i] for i in range(5))

# ==========================================
# 2. XManifoldUltra (96->96 å›å½’æ”¹é€ )
# ==========================================
class XManifoldUltra(nn.Module):
    def __init__(self, seq_len=96, pred_len=96, num_vars=14, d_model=256, noise_std=0.01):
        super().__init__()
        self.noise_std, self.revin = noise_std, RevIN(num_vars)
        self.experts = IntraColumnPentaExperts(seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model, 8, d_model*4, 0.2, batch_first=True, norm_first=True)
        self.game_room = nn.TransformerEncoder(encoder_layer, num_layers=6) 
        
        # ğŸ”‘ ä¿®å¤ 1ï¼šå°†å›å½’å¤´çš„ Dropout ä» 0.45 æš´é™åˆ° 0.1
        self.head = nn.Sequential(
            nn.Linear(num_vars * d_model, 512), 
            nn.GELU(), 
            nn.Dropout(0.1), 
            nn.Linear(512, pred_len)
        )
        
    def forward(self, x):
        # 1. å±€éƒ¨å½’ä¸€åŒ–
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        tokens = self.experts(x.permute(0, 2, 1).reshape(B * N, L)).view(B, N, -1)
        if self.training: tokens = tokens + torch.randn_like(tokens) * self.noise_std
        
        # 2. é¢„æµ‹è¾“å‡º [Batch, 96]
        out = self.head(self.game_room(tokens).reshape(B, -1))
        
        # ğŸ”‘ ä¿®å¤ 2ï¼šé’ˆå¯¹ç¬¬ 0 åˆ—(ç”µä»·)è¿›è¡Œé€†å½’ä¸€åŒ– (Denorm)
        out = out.unsqueeze(-1) # å˜æˆ [Batch, 96, 1]
        if self.revin.affine:
            out = (out - self.revin.affine_bias[0]) / self.revin.affine_weight[0]
        # ä½¿ç”¨ç¬¬ 0 åˆ—çš„ std å’Œ mean è¿˜åŸå°ºåº¦
        out = out * self.revin.stdev[:, :, 0:1] + self.revin.mean[:, :, 0:1]
        
        return out.squeeze(-1) # æ¢å¤æˆ [Batch, 96]

# ==========================================
# 3. å•è¡¨è¯»å–ç‰ˆ Dataset (è¯»å– spain_energy.csv)
# ==========================================
class SpainDataset(Dataset):
    def __init__(self, csv_path, flag='train', seq_len=96, pred_len=96, scaler=None):
        df = pd.read_csv(csv_path)
        
        # ä¸¥æ ¼æŒ‰ç…§åˆå¹¶åçš„åˆ—åæå– (12ä¸ªåŸºç¡€ç‰¹å¾)
        cols = ['price actual', 'total load actual', 'generation wind onshore', 
                'generation solar', 'generation fossil gas', 'generation nuclear', 
                'generation fossil hard coal', 'temp', 'pressure', 'humidity', 
                'wind_speed', 'clouds_all']
        
        df_feat = df[cols].copy()
        
        # åŠ ä¸Šæ—¶é—´ç‰¹å¾ (2ä¸ª) -> å…±è®¡ 14 ä¸ªç‰¹å¾
        df['date'] = pd.to_datetime(df['date'])
        hour = df['date'].dt.hour
        df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
        n = len(df_feat)
        tr, vl = int(n * 0.8), int(n * 0.9)
        
        if flag == 'train':
            feat_raw = df_feat.iloc[:tr].values
            self.scaler = StandardScaler().fit(feat_raw)
        elif flag == 'val':
            feat_raw = df_feat.iloc[tr-seq_len:vl].values
            self.scaler = scaler
        else:
            feat_raw = df_feat.iloc[vl-seq_len:].values
            self.scaler = scaler
            
        self.feat = self.scaler.transform(feat_raw)
        self.seq_len = seq_len
        self.pred_len = pred_len

    def __len__(self): 
        # ç•™è¶³é¢„æµ‹çª—å£ï¼Œé˜²æ­¢è¶Šç•Œ
        return len(self.feat) - self.seq_len - self.pred_len + 1
        
    def __getitem__(self, i): 
        x = self.feat[i : i+self.seq_len]
        # æå–ç›®æ ‡ï¼šæœªæ¥ 96 æ­¥çš„ 'price actual' (ç¬¬ 0 åˆ—)
        y = self.feat[i+self.seq_len : i+self.seq_len+self.pred_len, 0]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# ==========================================
# 4. ä¸»è®­ç»ƒæµ (DDP ä¿®å¤ & å›å½’æŒ‡æ ‡) 
# ==========================================
def main():
    dist.init_process_group("nccl")
    rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    # å‡è®¾ spain_energy.csv åœ¨è¿è¡Œç›®å½•ä¸‹
    csv_file = 'spain_energy.csv'
    train_ds = SpainDataset(csv_file, 'train')
    val_ds = SpainDataset(csv_file, 'val', scaler=train_ds.scaler)
    test_ds = SpainDataset(csv_file, 'test', scaler=train_ds.scaler)
    
    # é‰´äº XManifold æä¸ºåºå¤§ï¼Œbatch_size è®¾ä¸º 32
    train_loader = DataLoader(train_ds, 32, sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, 32, sampler=DistributedSampler(val_ds, shuffle=False))
    test_loader = DataLoader(test_ds, 32, sampler=DistributedSampler(test_ds, shuffle=False))

    model = DDP(XManifoldUltra(num_vars=14, seq_len=96, pred_len=96).to(device), 
                device_ids=[local_rank], find_unused_parameters=False)
                
    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)
    scaler = GradScaler()
    
    # å›å½’ä»»åŠ¡æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    mae_criterion = nn.L1Loss()

    best_val_loss, best_path = float('inf'), "x_best_96.pth"
    if rank == 0: print("ğŸš€ XManifold Ultra (96->96 å›å½’å¯¹å†³ç‰ˆ) å¯åŠ¨...")

    for epoch in range(20):
        model.train()
        train_loader.sampler.set_epoch(epoch)
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            with autocast():
                loss = criterion(model(bx), by)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        scheduler.step()

        # éªŒè¯é›†è¯„ä¼°
        model.eval()
        local_val_loss, local_total = 0.0, 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                pred = model(bx)
                local_val_loss += criterion(pred, by).item() * by.size(0)
                local_total += by.size(0)
                
        stats = torch.tensor([local_val_loss, local_total], device=device)
        dist.all_reduce(stats, op=dist.ReduceOp.SUM) 
        
        if rank == 0:
            val_loss = stats[0].item() / stats[1].item()
            if val_loss < best_val_loss: 
                best_val_loss = val_loss
                torch.save(model.module.state_dict(), best_path)
                print(f"Epoch {epoch+1:02d} Val MSE Loss: {val_loss:.4f} ğŸŒŸ (New Best)")
            else:
                print(f"Epoch {epoch+1:02d} Val MSE Loss: {val_loss:.4f} | Best remains: {best_val_loss:.4f}")

    dist.barrier()
    
    # ä¿®å¤ DDP Bug: è®©æ‰€æœ‰è¿›ç¨‹åŒæ­¥åŠ è½½ Best Weights
    model.module.load_state_dict(torch.load(best_path, map_location=device))
    if rank == 0:
        print(f"\nğŸ¯ è®­ç»ƒç»“æŸï¼Œæœ€ä¼˜æƒé‡å·²åŒæ­¥ï¼Œå¼€å§‹å…¨é‡æµ‹è¯•...")
        
    model.eval()
    local_test_mse, local_test_mae, local_total = 0.0, 0.0, 0
    with torch.no_grad():
        for bx, by in test_loader:
            bx, by = bx.to(device), by.to(device)
            pred = model(bx)
            local_test_mse += criterion(pred, by).item() * by.size(0)
            local_test_mae += mae_criterion(pred, by).item() * by.size(0)
            local_total += by.size(0)
            
    test_stats = torch.tensor([local_test_mse, local_test_mae, local_total], device=device)
    dist.all_reduce(test_stats, op=dist.ReduceOp.SUM)
    
    if rank == 0:
        final_mse = test_stats[0].item() / test_stats[2].item()
        final_mae = test_stats[1].item() / test_stats[2].item()
        print(f"==================================================")
        print(f"ğŸŒŸ Final XManifold Ultra Test MSE : {final_mse:.4f}")
        print(f"ğŸŒŸ Final XManifold Ultra Test MAE : {final_mae:.4f}")
        print(f"==================================================")
        
    dist.destroy_process_group()

if __name__ == "__main__": main()

ğŸ¯ è®­ç»ƒç»“æŸï¼Œæœ€ä¼˜æƒé‡å·²åŒæ­¥ï¼Œå¼€å§‹å…¨é‡æµ‹è¯•...
==================================================
ğŸŒŸ Final XManifold Ultra Test MSE : 0.1529
ğŸŒŸ Final XManifold Ultra Test MAE : 0.3030
==================================================


#itransformerç›´æ¥è°ƒç”¨å®˜æ–¹åº“ï¼š
# 1. å…‹éš†å®˜æ–¹ä»£ç åº“
git clone https://github.com/thuml/iTransformer.git
# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd iTransformer
# 3. å®‰è£…å®˜æ–¹æŒ‡å®šçš„ä¾èµ–åŒ…
pip install -r requirements.txt



(itrans) root@ubuntu22:~/iTransformer/iTransformer# python -u run.py \
  --is_training 1 \
  --root_path ./dataset/ \
  --data_path spain_energy.csv \
  --model_id Spain_96_96_FullRun \
  --model iTransformer \
  --data custom \
  --features MS \
  --seq_len 96 \
  --label_len 48 \
  --pred_len 96 \
  --e_layers 2 \
  --d_model 128 \
  --enc_in 12 \
  --dec_in 12 \
  --c_out 1 \
  --target 'price actual' \
  --des 'Exp' \
  --itr 1 \
  --train_epochs 20 \
  --patience 20 \
  --learning_rate 0.0005


Updating learning rate to 9.5367431640625e-10
>>>>>>>testing : Spain_96_96_FullRun_iTransformer_custom_MS_ft96_sl48_ll96_pl128_dm8_nh2_el1_dl2048_df1_fctimeF_ebTrue_dtExp_projection_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 6917
test shape: (6917, 1, 96, 1) (6917, 1, 96, 1)
test shape: (6917, 96, 1) (6917, 96, 1)
mse:0.20834733545780182, mae:0.315316379070282
