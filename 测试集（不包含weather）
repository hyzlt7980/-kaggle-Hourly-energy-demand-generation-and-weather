import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.amp import GradScaler, autocast
from sklearn.preprocessing import StandardScaler
from argparse import Namespace

# ==========================================
# 0. Ë∑ØÂæÑÂº∫Âà∂Ê≥®ÂÖ• (Á°Æ‰øù DDP Â≠êËøõÁ®ã 100% ÊâæÂà∞ models)
# ==========================================
ABS_PATH = "/root/official_it" 
if ABS_PATH not in sys.path:
    sys.path.insert(0, ABS_PATH)

try:
    from models.iTransformer import Model as iTransformer_Official
    print(f"‚úÖ [Rank {os.environ.get('RANK', '0')}] ÊàêÂäüÈìæÊé•ÂÆòÊñπ GitHub Ê∫êÁ†Å")
except ImportError as e:
    print(f"‚ùå [Rank {os.environ.get('RANK', '0')}] ÂØºÂÖ•Â§±Ë¥•: {e}")
    sys.exit(1)

# ==========================================
# 1. ÂÆòÊñπÊ®°ÂûãÈÄÇÈÖçÂô® (È´òÁª¥ÁâπÂæÅÊèêÂèñÁâà)
# ==========================================
class iTransformerClassifier(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.model = iTransformer_Official(configs)
        
        # ‰øÆÊ≠£Áª¥Â∫¶Ôºö9‰∏™ÂèòÈáè * ÊØè‰∏™ÂèòÈáè128Áª¥ = 1152
        self.feature_dim = configs.enc_in * configs.d_model
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.feature_dim, 512),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(512, 1)
        )

    def forward(self, x):
        # ÁªïËøáÂÆòÊñπÂõûÂΩí Projection Â±ÇÔºåÁõ¥Êé•ÊèêÂèñ Encoder ÂêéÁöÑÈ´òÁª¥ÁâπÂæÅ
        # x ÂΩ¢Áä∂: [Batch, Seq_Len, Vars]
        
        # 1. Inverted Embedding: [B, L, N] -> [B, N, D]
        enc_out = self.model.enc_embedding(x, None) 
        
        # 2. Transformer Encoder: [B, N, D] -> [B, N, D]
        enc_out, attns = self.model.encoder(enc_out, attn_mask=None)
        
        # 3. ÂàÜÁ±ªËæìÂá∫: [B, N*D] -> [B, 1]
        return self.classifier(enc_out)

# ==========================================
# 2. Êï∞ÊçÆÂºïÊìé (9ÂèòÈáèÔºå8:1:1 ÂàáÂàÜ)
# ==========================================
class SpainDataset(Dataset):
    def __init__(self, csv_path, flag='train', seq_len=96, scaler=None):
        df = pd.read_csv(csv_path)
        actual_cols = ['price actual', 'total load actual', 'generation wind onshore', 
                       'generation solar', 'generation fossil gas', 'generation nuclear', 
                       'generation fossil hard coal']
        
        # ÊªûÂêé 1 ‰ΩçÁ°Æ‰øùÈõ∂Ê≥ÑÈú≤
        df_feat = df[actual_cols].shift(1).ffill().bfill()
        t = pd.to_datetime(df['time'], utc=True) if 'time' in df.columns else pd.Series(range(len(df)))
        hour = t.dt.hour if hasattr(t, 'dt') else pd.Series(np.arange(len(df)) % 24)
        df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
        prices = pd.to_numeric(df['price actual'], errors='coerce').interpolate().values
        labels = (prices[1:] > prices[:-1]).astype(float)
        
        n = len(df_feat) - 1
        tr, vl = int(n * 0.8), int(n * 0.9)
        
        if flag == 'train':
            feat_raw, self.labels = df_feat.iloc[:tr].values, labels[:tr]
            self.scaler = StandardScaler().fit(feat_raw)
        elif flag == 'val':
            feat_raw, self.labels = df_feat.iloc[tr-seq_len:vl].values, labels[tr-seq_len-1:vl-1]
            self.scaler = scaler
        else:
            feat_raw, self.labels = df_feat.iloc[vl-seq_len:].values, labels[vl-seq_len-1:]
            self.scaler = scaler
            
        self.feat, self.seq_len = self.scaler.transform(feat_raw), seq_len

    def __len__(self): return len(self.feat) - self.seq_len
    def __getitem__(self, i): 
        return torch.tensor(self.feat[i:i+self.seq_len], dtype=torch.float32), \
               torch.tensor([self.labels[i+self.seq_len-1]], dtype=torch.float32)

# ==========================================
# 3. ËÆ≠ÁªÉ‰∏ªÁ®ãÂ∫è (DDP + 4Âç°4090‰ºòÂåñ)
# ==========================================
def main():
    dist.init_process_group("nccl")
    rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    # ÂÖ®ÂèÇÊï∞Ê≥®ÂÖ•ÔºåÁ°Æ‰øùÂÆòÊñπ‰ª£Á†ÅËøêË°å
    configs = Namespace(
        task_name='short_term_forecast',
        seq_len=96, label_len=48, pred_len=1,
        enc_in=9, dec_in=9, c_out=9,
        d_model=128, d_ff=256, n_heads=8, e_layers=3,
        dropout=0.1, activation='gelu', output_attention=False,
        use_norm=True, class_strategy='projection',
        embed='timeF', freq='h', factor=1, distil=True,
        moving_avg=25, p_hidden_dims=[128, 128], p_hidden_layers=2
    )

    CSV_NAME = 'energy_dataset.csv'
    train_ds = SpainDataset(CSV_NAME, 'train')
    val_ds = SpainDataset(CSV_NAME, 'val', scaler=train_ds.scaler)
    test_ds = SpainDataset(CSV_NAME, 'test', scaler=train_ds.scaler)
    
    train_loader = DataLoader(train_ds, 128, sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, 128, sampler=DistributedSampler(val_ds, shuffle=False))
    test_loader = DataLoader(test_ds, 128, sampler=DistributedSampler(test_ds, shuffle=False))

    # üëá ËøôÈáåÁöÑÁº©ËøõÂ∑≤Áªè‰øÆÂ§çÂØπÈΩê‰∫Ü
    model = iTransformerClassifier(configs).to(device)
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)
    
    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
    scaler = GradScaler()
    criterion = nn.BCEWithLogitsLoss()

    best_val_acc, best_path = 0.0, "it_official_9var_final.pth"
    if rank == 0: print(f"üöÄ iTransformer ÂÆòÊñπÊ†∏ÂøÉÂØπÂÜ≥ÁâàÂêØÂä® | ÂèòÈáèTokenÁª¥Â∫¶: {configs.d_model}")

    for epoch in range(20):
        model.train(); train_loader.sampler.set_epoch(epoch)
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            with autocast('cuda'):
                logits = model(bx)
                loss = criterion(logits, by * 0.9 + 0.05)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        scheduler.step()

        # È™åËØÅÁéØËäÇ
        model.eval(); correct, total = 0, 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                correct += ((model(bx) > 0).float() == by).sum().item()
                total += by.size(0)
        
        stats = torch.tensor([correct, total], device=device); dist.all_reduce(stats)
        if rank == 0:
            acc = stats[0].item() / stats[1].item()
            if acc > best_val_acc: 
                best_val_acc = acc
                torch.save(model.module.state_dict(), best_path)
            print(f"Epoch {epoch+1:02d} Val Acc: {acc:.2%} | Best: {best_val_acc:.2%}")

    dist.barrier()
    if rank == 0:
        print(f"\nüéØ ËÆ≠ÁªÉÁªìÊùüÔºåËØÑ‰º∞ÊúÄÁªà Test Set...")
        model.module.load_state_dict(torch.load(best_path, map_location=device))
        
        model.eval(); t_c, t_t = 0, 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.to(device), by.to(device)
                t_c += ((model(bx) > 0).float() == by).sum().item(); t_t += by.size(0)
        
        # Ê≥®ÊÑèËøôÈáåÊàëÊääÊâìÂç∞Êå™Âà∞‰∫Ü if rank == 0 ÈáåÈù¢ÔºåÂπ∂‰∏îÊ≤°ÊúâÂÜçÂÅö all_reduceÔºå
        # Âõ†‰∏∫Âú®ÊµãËØïÈò∂ÊÆµÂè™Áî®ÂçïÂç°Ë∑ë‰∏ÄÈÅçÊµãËØïÈõÜÊõ¥Á®≥Â¶•ÔºàÊàñËÄÖÂÅö all_reduce ÂêéÁ°Æ‰øùÊâÄÊúâ rank ÈÉΩÂú®Ôºâ„ÄÇ
        # ËøôÈáåÈááÁî®ÊúÄÁ®≥ÁöÑÂÜôÊ≥ïÔºöÂè™Âú® rank 0 ‰∏äÊ±áÊÄª„ÄÇ
        # ‰∏ä‰∏ÄÁâàËøôÈáåÁº©ËøõÊúâÈóÆÈ¢òÔºåÂèØËÉΩÂØºËá¥ rank 0 Ë∑ëÂà∞‰∏ÄÂçäÂç°‰Ωè„ÄÇ
        
    # ‰∏∫‰∫Ü‰øùËØÅÊâÄÊúâÊòæÂç°ÂêåÊ≠•ÈÄÄÂá∫
    dist.barrier()
    if rank == 0:
        print(f"üåü Final Official Test Acc: {t_c/t_t:.2%}")
    dist.destroy_process_group()

if __name__ == "__main__":
    main()

base) root@ubuntu22:~# torchrun --standalone --nproc_per_node=4 --master_port=29580 9v-i.py 
W0225 12:39:25.821000 10487 anaconda3/lib/python3.11/site-packages/torch/distributed/run.py:852] 
W0225 12:39:25.821000 10487 anaconda3/lib/python3.11/site-packages/torch/distributed/run.py:852] *****************************************
W0225 12:39:25.821000 10487 anaconda3/lib/python3.11/site-packages/torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0225 12:39:25.821000 10487 anaconda3/lib/python3.11/site-packages/torch/distributed/run.py:852] *****************************************
‚úÖ [Rank 0] ÊàêÂäüÈìæÊé•ÂÆòÊñπ GitHub Ê∫êÁ†Å
‚úÖ [Rank 3] ÊàêÂäüÈìæÊé•ÂÆòÊñπ GitHub Ê∫êÁ†Å
‚úÖ [Rank 2] ÊàêÂäüÈìæÊé•ÂÆòÊñπ GitHub Ê∫êÁ†Å
‚úÖ [Rank 1] ÊàêÂäüÈìæÊé•ÂÆòÊñπ GitHub Ê∫êÁ†Å
üöÄ iTransformer ÂÆòÊñπÊ†∏ÂøÉÂØπÂÜ≥ÁâàÂêØÂä® | ÂèòÈáèTokenÁª¥Â∫¶: 128
Epoch 01 Val Acc: 70.44% | Best: 70.44%
Epoch 02 Val Acc: 70.70% | Best: 70.70%
Epoch 03 Val Acc: 71.92% | Best: 71.92%
Epoch 04 Val Acc: 72.15% | Best: 72.15%
Epoch 05 Val Acc: 71.69% | Best: 72.15%
Epoch 06 Val Acc: 72.41% | Best: 72.41%
Epoch 07 Val Acc: 70.47% | Best: 72.41%
Epoch 08 Val Acc: 72.35% | Best: 72.41%
Epoch 09 Val Acc: 71.78% | Best: 72.41%
Epoch 10 Val Acc: 72.06% | Best: 72.41%
Epoch 11 Val Acc: 72.66% | Best: 72.66%
Epoch 12 Val Acc: 70.90% | Best: 72.66%
Epoch 13 Val Acc: 70.87% | Best: 72.66%
Epoch 14 Val Acc: 72.75% | Best: 72.75%
Epoch 15 Val Acc: 71.38% | Best: 72.75%
Epoch 16 Val Acc: 71.27% | Best: 72.75%
Epoch 17 Val Acc: 72.01% | Best: 72.75%
Epoch 18 Val Acc: 71.84% | Best: 72.75%
Epoch 19 Val Acc: 71.18% | Best: 72.75%
Epoch 20 Val Acc: 71.27% | Best: 72.75%
/root/anaconda3/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)

üéØ ËÆ≠ÁªÉÁªìÊùüÔºåËØÑ‰º∞ÊúÄÁªà Test Set...
üåü Final Official Test Acc: 71.84%




























#ËøôÂùóÊòØxmanifoldÂú®spain datasetÁöÑÁªìÊûú
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. Âü∫Á°ÄÁªÑ‰ª∂
# ==========================================
class Transpose(nn.Module):
    def __init__(self, dim0, dim1):
        super().__init__()
        self.dim0, self.dim1 = dim0, dim1
    def forward(self, x): return x.transpose(self.dim0, self.dim1)

class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps, self.affine = eps, affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode: str):
        if mode == 'norm':
            self.mean = x.mean(1, keepdim=True).detach()
            self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / self.affine_weight
            x = x * self.stdev + self.mean
        return x

# ==========================================
# 1. Swin 1D Ê†∏ÂøÉÈÄªËæë
# ==========================================
class SwinWindowAttention1D(nn.Module):
    def __init__(self, dim, window_size, shift_size, num_heads=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size, self.num_heads = dim, window_size, shift_size, num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)
        self.rel_pos_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1), num_heads))
        nn.init.trunc_normal_(self.rel_pos_bias_table, std=.02)

    def forward(self, x, mask=None):
        B_N, num_win, ws, D = x.shape
        qkv = self.qkv(x).reshape(B_N, num_win, ws, 3, self.num_heads, D // self.num_heads).permute(3, 0, 1, 4, 2, 5)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        coords = torch.arange(ws); rel_index = (coords[None, :] - coords[:, None] + ws - 1).to(x.device)
        rel_pos_bias = self.rel_pos_bias_table[rel_index.view(-1)].view(ws, ws, -1).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)
        attn = attn + rel_pos_bias
        if mask is not None: attn = attn + mask.unsqueeze(1).unsqueeze(0)
        attn = F.softmax(attn, dim=-1)
        return self.proj((attn @ v).transpose(2, 3).reshape(B_N, num_win, ws, D))

class SwinBlock1D(nn.Module):
    def __init__(self, dim, seq_len, window_size=8, shift_size=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size = dim, window_size, shift_size
        self.norm1 = nn.LayerNorm(dim); self.attn = SwinWindowAttention1D(dim, window_size, 0)
        self.norm2 = nn.LayerNorm(dim); self.shift_attn = SwinWindowAttention1D(dim, window_size, shift_size)
        if self.shift_size > 0:
            img_mask = torch.zeros((1, seq_len, 1)); s_slices = (slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))
            for i, s in enumerate(s_slices): img_mask[:, s, :] = i
            m_win = img_mask.view(1, seq_len // window_size, window_size, 1).reshape(-1, window_size)
            attn_mask = m_win.unsqueeze(1) - m_win.unsqueeze(2)
            self.register_buffer("attn_mask", attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0))
        else: self.attn_mask = None

    def forward(self, x):
        B_N, L, D = x.shape
        h = x; x = self.norm1(x).view(B_N, L // self.window_size, self.window_size, D)
        x = self.attn(x).view(B_N, L, D) + h
        h = x; x = self.norm2(x)
        if self.shift_size > 0: x = torch.roll(x, shifts=-self.shift_size, dims=1)
        x = x.view(B_N, L // self.window_size, self.window_size, D)
        x = self.shift_attn(x, mask=self.attn_mask).view(B_N, L, D)
        if self.shift_size > 0: x = torch.roll(x, shifts=self.shift_size, dims=1)
        return x + h

# ==========================================
# 2. ‰∫î‰∏ìÂÆ∂Èó®ÊéßÁ≥ªÁªü
# ==========================================
class IntraColumnPentaExperts(nn.Module):
    def __init__(self, seq_len=96, d_model=256, path_drop=0.35):
        super().__init__()
        self.path_drop = path_drop
        self.exp_global = nn.Linear(seq_len, d_model)
        self.exp_local = nn.Sequential(nn.Conv1d(1, 64, 3, padding=1), nn.GELU(), nn.Conv1d(64, 1, 3, padding=1), nn.Flatten(), nn.Linear(seq_len, d_model), nn.Dropout(0.5))
        self.exp_diff = nn.Sequential(nn.Linear(seq_len - 1, d_model), nn.LayerNorm(d_model), nn.Dropout(0.45))
        self.exp_swin = nn.Sequential(nn.Linear(1, d_model), SwinBlock1D(d_model, seq_len), Transpose(1, 2), nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Dropout(0.45))
        self.exp_sliding = nn.Sequential(nn.Conv1d(1, d_model, kernel_size=3, stride=1, padding=1), nn.BatchNorm1d(d_model), nn.GELU(), nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Dropout(0.45))
        self.gate = nn.Sequential(nn.Linear(d_model * 5, 128), nn.GELU(), nn.Linear(128, 5))
        self.temp = 2.0 

    def forward(self, x_strip):
        f1 = self.exp_global(x_strip)
        f2 = self.exp_local(x_strip.unsqueeze(1))
        f3 = self.exp_diff(x_strip[:, 1:] - x_strip[:, :-1])
        f4 = self.exp_swin(x_strip.unsqueeze(-1))
        f5 = self.exp_sliding(x_strip.unsqueeze(1))
        experts = [f1, f2, f3, f4, f5]
        if self.training:
            for i in range(len(experts)):
                if torch.rand(1) < self.path_drop: experts[i] = torch.zeros_like(experts[i])
        logits = self.gate(torch.cat([f1, f2, f3, f4, f5], dim=-1))
        w = F.softmax(logits / self.temp, dim=-1)
        return sum(w[:, i:i+1] * experts[i] for i in range(5))

# ==========================================
# 3. XManifoldUltra Ê†∏ÂøÉÊû∂ÊûÑ
# ==========================================
class XManifoldUltra(nn.Module):
    def __init__(self, seq_len=96, num_vars=14, d_model=256, noise_std=0.01):
        super().__init__()
        self.noise_std, self.revin = noise_std, RevIN(num_vars)
        self.experts = IntraColumnPentaExperts(seq_len, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, 8, d_model*4, 0.2, batch_first=True, norm_first=True)
        self.game_room = nn.TransformerEncoder(encoder_layer, 3)
        self.head = nn.Sequential(nn.Linear(num_vars * d_model, 512), nn.GELU(), nn.Dropout(0.45), nn.Linear(512, 1))

    def forward(self, x):
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        tokens = self.experts(x.permute(0, 2, 1).reshape(B * N, L)).view(B, N, -1)
        if self.training: tokens = tokens + torch.randn_like(tokens) * self.noise_std
        return self.head(self.game_room(tokens).reshape(B, -1))

# ==========================================
# 4. Êï∞ÊçÆÂä†ËΩΩ (ÂØπÈΩê iTransformer)
# ==========================================
class SpainDataset(Dataset):
    def __init__(self, energy_path, weather_path, flag='train', seq_len=96, scaler=None):
        df_e, df_w = pd.read_csv(energy_path), pd.read_csv(weather_path)
        df_e['time'] = pd.to_datetime(df_e['time'], utc=True)
        df_w['time'] = pd.to_datetime(df_w['dt_iso'], utc=True)
        w_cols = ['temp', 'pressure', 'humidity', 'wind_speed', 'clouds_all']
        df_w[w_cols] = df_w[w_cols].apply(pd.to_numeric, errors='coerce')
        w_avg = df_w.groupby('time')[w_cols].mean().reset_index()
        full_df = pd.merge(df_e, w_avg, on='time', how='left')
        
        actual_cols = ['price actual', 'total load actual', 'generation wind onshore', 'generation solar', 
                       'generation fossil gas', 'generation nuclear', 'generation fossil hard coal'] + w_cols
        df_feat = full_df[actual_cols].shift(1).ffill().bfill()
        hour = full_df['time'].dt.hour
        df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
        prices = pd.to_numeric(full_df['price actual'], errors='coerce').interpolate().values
        labels = (prices[1:] > prices[:-1]).astype(float)
        
        n = len(df_feat) - 1
        tr, vl = int(n * 0.8), int(n * 0.9)
        if flag == 'train':
            feat_raw, self.labels = df_feat.iloc[:tr].values, labels[:tr]
            self.scaler = StandardScaler().fit(feat_raw)
        elif flag == 'val':
            feat_raw, self.labels = df_feat.iloc[tr-seq_len:vl].values, labels[tr-seq_len-1:vl-1]
            self.scaler = scaler
        else:
            feat_raw, self.labels = df_feat.iloc[vl-seq_len:].values, labels[vl-seq_len-1:]
            self.scaler = scaler
        self.feat, self.seq_len = self.scaler.transform(feat_raw), seq_len

    def __len__(self): return len(self.feat) - self.seq_len
    def __getitem__(self, i): return torch.tensor(self.feat[i:i+self.seq_len], dtype=torch.float32), torch.tensor([self.labels[i+self.seq_len-1]], dtype=torch.float32)

# ==========================================
# 5. DDP ËÆ≠ÁªÉÁ≥ªÁªü
# ==========================================
def main():
    dist.init_process_group("nccl")
    rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    train_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'train')
    val_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'val', scaler=train_ds.scaler)
    test_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'test', scaler=train_ds.scaler)
    
    train_loader = DataLoader(train_ds, 128, sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, 128, sampler=DistributedSampler(val_ds, shuffle=False))
    test_loader = DataLoader(test_ds, 128, sampler=DistributedSampler(test_ds, shuffle=False))

    model = DDP(XManifoldUltra(num_vars=14).to(device), device_ids=[local_rank])
    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
    scaler = GradScaler()
    criterion = nn.BCEWithLogitsLoss()

    best_val_acc, best_path = 0.0, "x_best.pth"
    if rank == 0: print("üöÄ XManifoldUltra ÂØπÂÜ≥ÁâàÂêØÂä®...")

    for epoch in range(20):
        model.train(); train_loader.sampler.set_epoch(epoch)
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            with autocast(): # üëà ‰øÆÂ§çÊ≠§Â§ÑÂèÇÊï∞Êä•Èîô
                logits = model(bx)
                loss = criterion(logits, by * 0.9 + 0.05)
            scaler.scale(loss).backward()
            scaler.step(optimizer); scaler.update()
        scheduler.step()

        model.eval(); correct, total = 0, 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                correct += ((model(bx) > 0).float() == by).sum().item()
                total += by.size(0)
        stats = torch.tensor([correct, total], device=device)
        dist.all_reduce(stats)
        if rank == 0:
            acc = stats[0].item() / stats[1].item()
            if acc > best_val_acc: best_val_acc = acc; torch.save(model.module.state_dict(), best_path)
            print(f"Epoch {epoch+1} Val Acc: {acc:.2%}")

    dist.barrier()
    if rank == 0:
        model.module.load_state_dict(torch.load(best_path))
        model.eval(); t_c, t_t = 0, 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.to(device), by.to(device)
                t_c += ((model(bx) > 0).float() == by).sum().item()
                t_t += by.size(0)
        print(f"üåü XManifold Final Test Acc: {t_c/t_t:.2%}")
    dist.destroy_process_group()

if __name__ == "__main__": main()


(base) root@ubuntu22:~# torchrun --standalone --nproc_per_node=4 --master_port=29505 train1.py
[2026-02-25 11:46:58,761] torch.distributed.run: [WARNING] 
[2026-02-25 11:46:58,761] torch.distributed.run: [WARNING] *****************************************
[2026-02-25 11:46:58,761] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-25 11:46:58,761] torch.distributed.run: [WARNING] *****************************************
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
üöÄ XManifoldUltra Êª°Ë°Ä‰∫î‰∏ìÂÆ∂ÁâàÂêØÂä®...
/root/train1.py:193: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  return self.fc(torch.abs(x_fft))
/root/train1.py:193: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  return self.fc(torch.abs(x_fft))
/root/train1.py:193: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  return self.fc(torch.abs(x_fft))
[rank3]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
XManifold Ultra Epoch 01 Val Acc: 67.65% | Best: 67.65%
XManifold Ultra Epoch 02 Val Acc: 70.07% | Best: 70.07%
XManifold Ultra Epoch 03 Val Acc: 72.43% | Best: 72.43%
XManifold Ultra Epoch 04 Val Acc: 71.78% | Best: 72.43%
XManifold Ultra Epoch 05 Val Acc: 71.89% | Best: 72.43%
XManifold Ultra Epoch 06 Val Acc: 71.55% | Best: 72.43%
XManifold Ultra Epoch 07 Val Acc: 70.04% | Best: 72.43%
XManifold Ultra Epoch 08 Val Acc: 70.50% | Best: 72.43%
XManifold Ultra Epoch 09 Val Acc: 70.81% | Best: 72.43%
XManifold Ultra Epoch 10 Val Acc: 71.78% | Best: 72.43%
XManifold Ultra Epoch 11 Val Acc: 71.92% | Best: 72.43%
XManifold Ultra Epoch 12 Val Acc: 71.15% | Best: 72.43%
XManifold Ultra Epoch 13 Val Acc: 71.41% | Best: 72.43%
XManifold Ultra Epoch 14 Val Acc: 71.98% | Best: 72.43%
XManifold Ultra Epoch 15 Val Acc: 71.01% | Best: 72.43%
XManifold Ultra Epoch 16 Val Acc: 71.32% | Best: 72.43%
XManifold Ultra Epoch 17 Val Acc: 71.12% | Best: 72.43%
XManifold Ultra Epoch 18 Val Acc: 71.09% | Best: 72.43%
XManifold Ultra Epoch 19 Val Acc: 70.98% | Best: 72.43%
XManifold Ultra Epoch 20 Val Acc: 71.12% | Best: 72.43%

üéØ ËÆ≠ÁªÉÂúÜÊª°ÁªìÊùüÔºåÂºÄÂßãÊµãËØïÈõÜËØÑ‰º∞...
üåü Final XManifold Ultra Test Acc: 74.29%
