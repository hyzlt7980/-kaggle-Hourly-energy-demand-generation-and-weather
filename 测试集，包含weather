# import os
# import pandas as pd
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.utils.data import Dataset, DataLoader, DistributedSampler
# from sklearn.preprocessing import StandardScaler

# class RevIN(nn.Module):
#     def __init__(self, num_features, eps=1e-5, affine=True):
#         super().__init__()
#         self.eps, self.affine = eps, affine
#         if self.affine:
#             self.affine_weight = nn.Parameter(torch.ones(num_features))
#             self.affine_bias = nn.Parameter(torch.zeros(num_features))
#     def forward(self, x, mode: str):
#         if mode == 'norm':
#             self.mean = x.mean(1, keepdim=True).detach()
#             self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
#             x = (x - self.mean) / self.stdev
#             if self.affine: x = x * self.affine_weight + self.affine_bias
#         elif mode == 'denorm':
#             if self.affine: x = (x - self.affine_bias) / self.affine_weight
#             x = x * self.stdev + self.mean
#         return x

# class iTransformer_Official(nn.Module):
#     def __init__(self, n_vars, seq_len, d_model=128, n_heads=8, e_layers=3, d_ff=256, dropout=0.1):
#         super().__init__()
#         self.revin = RevIN(n_vars)
#         self.enc_embedding = nn.Linear(seq_len, d_model)
#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True, activation='gelu')
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)
#         self.head = nn.Sequential(nn.Flatten(), nn.Linear(n_vars * d_model, 256), nn.GELU(), nn.Linear(256, 1))
#     def forward(self, x):
#         x = self.revin(x, 'norm')
#         x = x.permute(0, 2, 1)
#         enc_out = self.encoder(self.enc_embedding(x))
#         return self.head(enc_out)

# class SpainDataset(Dataset):
#     def __init__(self, energy_path, weather_path, flag='train', seq_len=96, scaler=None):
#         df_e, df_w = pd.read_csv(energy_path), pd.read_csv(weather_path)
#         df_e['time'] = pd.to_datetime(df_e['time'], utc=True)
#         df_w['time'] = pd.to_datetime(df_w['dt_iso'], utc=True)
#         w_cols = ['temp', 'pressure', 'humidity', 'wind_speed', 'clouds_all']
#         df_w[w_cols] = df_w[w_cols].apply(pd.to_numeric, errors='coerce')
#         w_avg = df_w.groupby('time')[w_cols].mean().reset_index()
#         full_df = pd.merge(df_e, w_avg, on='time', how='left')
        
#         # 14‰∏™ÂØπÈΩêÁâπÂæÅ [Image of iTransformer and XManifold input alignment]
#         actual_cols = ['price actual', 'total load actual', 'generation wind onshore', 'generation solar', 
#                        'generation fossil gas', 'generation nuclear', 'generation fossil hard coal'] + w_cols
#         df_feat = full_df[actual_cols].shift(1).ffill().bfill()
#         hour = full_df['time'].dt.hour
#         df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
#         labels = (pd.to_numeric(full_df['price actual'], errors='coerce').interpolate().values[1:] > 
#                   pd.to_numeric(full_df['price actual'], errors='coerce').interpolate().values[:-1]).astype(float)
        
#         n = len(df_feat) - 1
#         tr, vl = int(n * 0.8), int(n * 0.9)
#         if flag == 'train':
#             feat_raw, self.labels = df_feat.iloc[:tr].values, labels[:tr]
#             self.scaler = StandardScaler().fit(feat_raw)
#         elif flag == 'val':
#             feat_raw, self.labels = df_feat.iloc[tr-seq_len:vl].values, labels[tr-seq_len-1:vl-1]
#             self.scaler = scaler
#         else:
#             feat_raw, self.labels = df_feat.iloc[vl-seq_len:].values, labels[vl-seq_len-1:]
#             self.scaler = scaler
#         self.feat, self.seq_len = self.scaler.transform(feat_raw), seq_len

#     def __len__(self): return len(self.feat) - self.seq_len
#     def __getitem__(self, i): return torch.tensor(self.feat[i:i+self.seq_len], dtype=torch.float32), torch.tensor([self.labels[i+self.seq_len-1]], dtype=torch.float32)

# def main():
#     dist.init_process_group("nccl")
#     rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
#     torch.cuda.set_device(local_rank)
#     device = torch.device("cuda", local_rank)

#     train_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'train')
#     val_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'val', scaler=train_ds.scaler)
#     test_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'test', scaler=train_ds.scaler)
    
#     train_loader = DataLoader(train_ds, 128, sampler=DistributedSampler(train_ds))
#     val_loader = DataLoader(val_ds, 128, sampler=DistributedSampler(val_ds, shuffle=False))
#     test_loader = DataLoader(test_ds, 128, sampler=DistributedSampler(test_ds, shuffle=False))

#     model = DDP(iTransformer_Official(n_vars=14, seq_len=96).to(device), device_ids=[local_rank])
#     optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)
#     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
#     criterion = nn.BCEWithLogitsLoss()

#     best_val_acc, best_path = 0.0, "it_best.pth"
#     for epoch in range(20):
#         model.train(); train_loader.sampler.set_epoch(epoch)
#         for bx, by in train_loader:
#             bx, by = bx.to(device), by.to(device)
#             optimizer.zero_grad()
#             criterion(model(bx), by * 0.9 + 0.05).backward()
#             optimizer.step()
#         scheduler.step()

#         model.eval(); correct, total = 0, 0
#         with torch.no_grad():
#             for bx, by in val_loader:
#                 bx, by = bx.to(device), by.to(device)
#                 correct += ((model(bx) > 0).float() == by).sum().item()
#                 total += by.size(0)
#         stats = torch.tensor([correct, total], device=device)
#         dist.all_reduce(stats)
#         if rank == 0:
#             acc = stats[0].item() / stats[1].item()
#             if acc > best_val_acc: best_val_acc = acc; torch.save(model.module.state_dict(), best_path)
#             print(f"Epoch {epoch+1} Val Acc: {acc:.2%}")

#     dist.barrier()
#     if rank == 0:
#         model.module.load_state_dict(torch.load(best_path))
#         model.eval(); t_c, t_t = 0, 0
#         with torch.no_grad():
#             for bx, by in test_loader:
#                 bx, by = bx.to(device), by.to(device)
#                 t_c += ((model(bx) > 0).float() == by).sum().item()
#                 t_t += by.size(0)
#         print(f"üåü iTransformer Final Test Acc: {t_c/t_t:.2%}")
#     dist.destroy_process_group()

# if __name__ == "__main__": main()


import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import StandardScaler

# ==========================================
# 0. Âü∫Á°ÄÁªÑ‰ª∂
# ==========================================
class Transpose(nn.Module):
    def __init__(self, dim0, dim1):
        super().__init__()
        self.dim0, self.dim1 = dim0, dim1
    def forward(self, x): return x.transpose(self.dim0, self.dim1)

class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps, self.affine = eps, affine
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    def forward(self, x, mode: str):
        if mode == 'norm':
            self.mean = x.mean(1, keepdim=True).detach()
            self.stdev = torch.sqrt(x.var(1, keepdim=True, unbiased=False) + self.eps).detach()
            x = (x - self.mean) / self.stdev
            if self.affine: x = x * self.affine_weight + self.affine_bias
        elif mode == 'denorm':
            if self.affine: x = (x - self.affine_bias) / self.affine_weight
            x = x * self.stdev + self.mean
        return x

# ==========================================
# 1. ÂºÇÊûÑ‰∫î‰∏ìÂÆ∂ÁªÑ‰ª∂ (ÁúüÊ≠£ÁªÜËäÇÁâà)
# ==========================================
class SpectralExpert(nn.Module):
    """‰∏ìÂÆ∂1: È¢ëÂüü‰∏ìÂÆ∂„ÄÇÂà©Áî®FFTÊçïÊçâÁîµÂäõÊï∞ÊçÆÁöÑÂº∫Âë®ÊúüÊÄßËßÑÂæã"""
    def __init__(self, seq_len, d_model):
        super().__init__()
        self.fc = nn.Linear(seq_len // 2 + 1, d_model)
    def forward(self, x):
        x_fft = torch.fft.rfft(x, dim=-1)
        return self.fc(torch.abs(x_fft))

class InceptionExpert(nn.Module):
    """‰∏ìÂÆ∂2: Â§öÂ∞∫Â∫¶‰∏ìÂÆ∂„ÄÇÂà©Áî®‰∏çÂêåÊÑüÂèóÈáéÊçïÊçâÂ±ÄÈÉ®Âíå‰∏≠ÊúüÁöÑÊ≥¢Âä®"""
    def __init__(self, d_model):
        super().__init__()
        self.c1 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=1)
        self.c2 = nn.Conv1d(1, d_model//4, kernel_size=7, padding=3)
        self.c3 = nn.Conv1d(1, d_model//4, kernel_size=11, padding=5)
        self.c4 = nn.Conv1d(1, d_model//4, kernel_size=3, padding=2, dilation=2)
        self.pool = nn.AdaptiveAvgPool1d(1)
    def forward(self, x):
        x = x.unsqueeze(1) # [B*N, 1, L]
        out = torch.cat([self.c1(x), self.c2(x), self.c3(x), self.c4(x)], dim=1)
        return self.pool(F.gelu(out)).flatten(1)



class SwinWindowAttention1D(nn.Module):
    def __init__(self, dim, window_size, shift_size, num_heads=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size, self.num_heads = dim, window_size, shift_size, num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.qkv = nn.Linear(dim, dim * 3); self.proj = nn.Linear(dim, dim)
        self.rel_pos_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1), num_heads))
        nn.init.trunc_normal_(self.rel_pos_bias_table, std=.02)
    def forward(self, x, mask=None):
        B_N, num_win, ws, D = x.shape
        qkv = self.qkv(x).reshape(B_N, num_win, ws, 3, self.num_heads, D // self.num_heads).permute(3, 0, 1, 4, 2, 5)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        coords = torch.arange(ws); rel_index = (coords[None, :] - coords[:, None] + ws - 1).to(x.device)
        rel_pos_bias = self.rel_pos_bias_table[rel_index.view(-1)].view(ws, ws, -1).permute(2, 0, 1).unsqueeze(0).unsqueeze(0)
        attn = attn + rel_pos_bias
        if mask is not None: attn = attn + mask.unsqueeze(1).unsqueeze(0)
        attn = F.softmax(attn, dim=-1)
        return self.proj((attn @ v).transpose(2, 3).reshape(B_N, num_win, ws, D))

class SwinBlock1D(nn.Module):
    def __init__(self, dim, seq_len, window_size=8, shift_size=4):
        super().__init__()
        self.dim, self.window_size, self.shift_size = dim, window_size, shift_size
        self.norm1 = nn.LayerNorm(dim); self.attn = SwinWindowAttention1D(dim, window_size, 0)
        self.norm2 = nn.LayerNorm(dim); self.shift_attn = SwinWindowAttention1D(dim, window_size, shift_size)
        if self.shift_size > 0:
            img_mask = torch.zeros((1, seq_len, 1))
            for i, s in enumerate((slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None))): img_mask[:, s, :] = i
            m_win = img_mask.view(1, seq_len // window_size, window_size, 1).reshape(-1, window_size)
            attn_mask = m_win.unsqueeze(1) - m_win.unsqueeze(2)
            self.register_buffer("attn_mask", attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0))
        else: self.attn_mask = None
    def forward(self, x):
        B_N, L, D = x.shape
        h = x; x = self.norm1(x).view(B_N, L // self.window_size, self.window_size, D)
        x = self.attn(x).view(B_N, L, D) + h
        h = x; x = self.norm2(x)
        if self.shift_size > 0: x = torch.roll(x, shifts=-self.shift_size, dims=1)
        x = x.view(B_N, L // self.window_size, self.window_size, D)
        x = self.shift_attn(x, mask=self.attn_mask).view(B_N, L, D)
        if self.shift_size > 0: x = torch.roll(x, shifts=self.shift_size, dims=1)
        return x + h

class IntraColumnPentaExperts(nn.Module):
    def __init__(self, seq_len=96, d_model=256, path_drop=0.35):
        super().__init__()
        self.path_drop = path_drop
        self.exp1_spectral = SpectralExpert(seq_len, d_model)
        self.exp2_inception = InceptionExpert(d_model)
        self.exp3_swin = nn.Sequential(nn.Linear(1, d_model), SwinBlock1D(d_model, seq_len), Transpose(1, 2), nn.AdaptiveAvgPool1d(1), nn.Flatten())
        self.exp4_diff = nn.Sequential(nn.Linear(seq_len - 1, d_model), nn.LayerNorm(d_model))
        self.exp5_global = nn.Sequential(nn.Linear(seq_len, d_model), nn.ReLU(), nn.Linear(d_model, d_model))
        self.gate = nn.Sequential(nn.Linear(d_model * 5, 256), nn.GELU(), nn.Linear(256, 5))
        self.temp = 2.0 
    def forward(self, x_strip):
        f1, f2 = self.exp1_spectral(x_strip), self.exp2_inception(x_strip)
        f3 = self.exp3_swin(x_strip.unsqueeze(-1))
        f4 = self.exp4_diff(x_strip[:, 1:] - x_strip[:, :-1])
        f5 = self.exp5_global(x_strip)
        experts = [f1, f2, f3, f4, f5]
        if self.training:
            for i in range(5):
                if torch.rand(1) < self.path_drop: experts[i] = torch.zeros_like(experts[i])
        w = F.softmax(self.gate(torch.cat(experts, dim=-1)) / self.temp, dim=-1)
        return sum(w[:, i:i+1] * experts[i] for i in range(5))



# ==========================================
# 2. XManifoldUltra Ê†∏ÂøÉÊû∂ÊûÑ
# ==========================================
class XManifoldUltra(nn.Module):
    def __init__(self, seq_len=96, num_vars=14, d_model=256, noise_std=0.01):
        super().__init__()
        self.noise_std, self.revin = noise_std, RevIN(num_vars)
        self.experts = IntraColumnPentaExperts(seq_len, d_model)
        # 6 Â±ÇÂçöÂºàÁ©∫Èó¥ÔºåÊçïÊçâÂèòÈáèÈó¥ÁöÑÈùûÁ∫øÊÄßÂÖ≥Á≥ª
        encoder_layer = nn.TransformerEncoderLayer(d_model, 8, d_model*4, 0.2, batch_first=True, norm_first=True)
        self.game_room = nn.TransformerEncoder(encoder_layer, num_layers=6) 
        self.head = nn.Sequential(nn.Linear(num_vars * d_model, 512), nn.GELU(), nn.Dropout(0.45), nn.Linear(512, 1))
    def forward(self, x):
        x = self.revin(x, 'norm')
        B, L, N = x.shape
        tokens = self.experts(x.permute(0, 2, 1).reshape(B * N, L)).view(B, N, -1)
        if self.training: tokens = tokens + torch.randn_like(tokens) * self.noise_std
        return self.head(self.game_room(tokens).reshape(B, -1))

# ==========================================
# 3. Êï∞ÊçÆÂºïÊìé (80/10/10)
# ==========================================
class SpainDataset(Dataset):
    def __init__(self, energy_path, weather_path, flag='train', seq_len=96, scaler=None):
        df_e, df_w = pd.read_csv(energy_path), pd.read_csv(weather_path)
        df_e['time'] = pd.to_datetime(df_e['time'], utc=True)
        df_w['time'] = pd.to_datetime(df_w['dt_iso'], utc=True)
        w_cols = ['temp', 'pressure', 'humidity', 'wind_speed', 'clouds_all']
        df_w[w_cols] = df_w[w_cols].apply(pd.to_numeric, errors='coerce')
        w_avg = df_w.groupby('time')[w_cols].mean().reset_index()
        full_df = pd.merge(df_e, w_avg, on='time', how='left')
        
        actual_cols = ['price actual', 'total load actual', 'generation wind onshore', 'generation solar', 
                       'generation fossil gas', 'generation nuclear', 'generation fossil hard coal'] + w_cols
        df_feat = full_df[actual_cols].shift(1).ffill().bfill()
        hour = full_df['time'].dt.hour
        df_feat['h_sin'], df_feat['h_cos'] = np.sin(2*np.pi*hour/24), np.cos(2*np.pi*hour/24)
        
        prices = pd.to_numeric(full_df['price actual'], errors='coerce').interpolate().values
        labels = (prices[1:] > prices[:-1]).astype(float)
        
        n = len(df_feat) - 1
        tr, vl = int(n * 0.8), int(n * 0.9)
        if flag == 'train':
            feat_raw, self.labels = df_feat.iloc[:tr].values, labels[:tr]
            self.scaler = StandardScaler().fit(feat_raw)
        elif flag == 'val':
            feat_raw, self.labels = df_feat.iloc[tr-seq_len:vl].values, labels[tr-seq_len-1:vl-1]
            self.scaler = scaler
        else:
            feat_raw, self.labels = df_feat.iloc[vl-seq_len:].values, labels[vl-seq_len-1:]
            self.scaler = scaler
        self.feat, self.seq_len = self.scaler.transform(feat_raw), seq_len
    def __len__(self): return len(self.feat) - self.seq_len
    def __getitem__(self, i): return torch.tensor(self.feat[i:i+self.seq_len], dtype=torch.float32), torch.tensor([self.labels[i+self.seq_len-1]], dtype=torch.float32)

# ==========================================
# 4. ËÆ≠ÁªÉ‰∏ªÁ®ãÂ∫è (Ëß£ÂÜ≥ÂêåÊ≠•Ê≠ªÈîÅ)
# ==========================================
def main():
    dist.init_process_group("nccl")
    rank, local_rank = int(os.environ["RANK"]), int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    train_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'train')
    val_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'val', scaler=train_ds.scaler)
    test_ds = SpainDataset('energy_dataset.csv', 'weather_features.csv', 'test', scaler=train_ds.scaler)
    
    train_loader = DataLoader(train_ds, 128, sampler=DistributedSampler(train_ds))
    val_loader = DataLoader(val_ds, 128, sampler=DistributedSampler(val_ds, shuffle=False))
    test_loader = DataLoader(test_ds, 128, sampler=DistributedSampler(test_ds, shuffle=False))

    model = DDP(XManifoldUltra(num_vars=14).to(device), device_ids=[local_rank], find_unused_parameters=True)
    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)
    scaler, criterion = GradScaler(), nn.BCEWithLogitsLoss()

    best_val_acc, best_path = 0.0, "x_best_v3.pth"
    if rank == 0: print("üöÄ XManifold Ultra Êª°Ë°ÄÁâàÂêØÂä®...")

    for epoch in range(20):
        model.train(); train_loader.sampler.set_epoch(epoch)
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            with autocast():
                loss = criterion(model(bx), by * 0.85 + 0.075)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        scheduler.step()

        model.eval(); correct, total = 0, 0
        with torch.no_grad():
            for bx, by in val_loader:
                bx, by = bx.to(device), by.to(device)
                correct += ((model(bx) > 0).float() == by).sum().item(); total += by.size(0)
        stats = torch.tensor([correct, total], device=device); dist.all_reduce(stats)
        if rank == 0:
            acc = stats[0].item() / stats[1].item()
            if acc > best_val_acc: best_val_acc = acc; torch.save(model.module.state_dict(), best_path)
            print(f"Epoch {epoch+1:02d} Val Acc: {acc:.2%} | Best: {best_val_acc:.2%}")

    dist.barrier()
    if rank == 0:
        print(f"\nüéØ ËÆ≠ÁªÉÁªìÊùüÔºåÂºÄÂßãÊúÄÁªàÊµãËØï...")
        model.module.load_state_dict(torch.load(best_path, map_location=device))
        
    model.eval(); t_c, t_t = 0, 0
    with torch.no_grad():
        for bx, by in test_loader:
            bx, by = bx.to(device), by.to(device)
            t_c += ((model(bx) > 0).float() == by).sum().item(); t_t += by.size(0)
    test_stats = torch.tensor([t_c, t_t], device=device); dist.all_reduce(test_stats)
    if rank == 0:
        print(f"üåü Final XManifold Ultra Test Acc: {test_stats[0].item()/test_stats[1].item():.2%}")
    dist.destroy_process_group()

if __name__ == "__main__": main()



(base) root@ubuntu22:~# torchrun --standalone --nproc_per_node=4 --master_port=29505 train2.py
[2026-02-25 11:52:34,546] torch.distributed.run: [WARNING] 
[2026-02-25 11:52:34,546] torch.distributed.run: [WARNING] *****************************************
[2026-02-25 11:52:34,546] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-25 11:52:34,546] torch.distributed.run: [WARNING] *****************************************
Epoch 1 Val Acc: 72.61%
Epoch 2 Val Acc: 72.41%
Epoch 3 Val Acc: 70.04%
Epoch 4 Val Acc: 71.15%
Epoch 5 Val Acc: 70.81%
Epoch 6 Val Acc: 71.07%
Epoch 7 Val Acc: 69.98%
Epoch 8 Val Acc: 70.13%
Epoch 9 Val Acc: 71.35%
Epoch 10 Val Acc: 71.49%
Epoch 11 Val Acc: 70.75%
Epoch 12 Val Acc: 69.95%
Epoch 13 Val Acc: 70.61%
Epoch 14 Val Acc: 70.55%
Epoch 15 Val Acc: 69.53%
Epoch 16 Val Acc: 70.04%
Epoch 17 Val Acc: 69.98%
Epoch 18 Val Acc: 70.07%
Epoch 19 Val Acc: 70.07%
Epoch 20 Val Acc: 70.04%
üåü iTransformer Final Test Acc: 66.59%
(base) root@ubuntu22:~# torchrun --standalone --nproc_per_node=4 --master_port=29505 train2.py
[2026-02-25 11:55:07,903] torch.distributed.run: [WARNING] 
[2026-02-25 11:55:07,903] torch.distributed.run: [WARNING] *****************************************
[2026-02-25 11:55:07,903] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2026-02-25 11:55:07,903] torch.distributed.run: [WARNING] *****************************************
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/root/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
üöÄ XManifold Ultra Êª°Ë°ÄÁâàÂêØÂä®...
[rank3]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 01 Val Acc: 67.30% | Best: 67.30%
Epoch 02 Val Acc: 70.04% | Best: 70.04%
Epoch 03 Val Acc: 72.55% | Best: 72.55%
Epoch 04 Val Acc: 71.09% | Best: 72.55%
Epoch 05 Val Acc: 71.66% | Best: 72.55%
Epoch 06 Val Acc: 70.18% | Best: 72.55%
Epoch 07 Val Acc: 71.38% | Best: 72.55%
Epoch 08 Val Acc: 70.67% | Best: 72.55%
Epoch 09 Val Acc: 71.44% | Best: 72.55%
Epoch 10 Val Acc: 70.78% | Best: 72.55%
Epoch 11 Val Acc: 72.04% | Best: 72.55%
Epoch 12 Val Acc: 69.58% | Best: 72.55%
Epoch 13 Val Acc: 70.84% | Best: 72.55%
Epoch 14 Val Acc: 71.29% | Best: 72.55%
Epoch 15 Val Acc: 70.70% | Best: 72.55%
Epoch 16 Val Acc: 70.15% | Best: 72.55%
Epoch 17 Val Acc: 70.92% | Best: 72.55%
Epoch 18 Val Acc: 70.30% | Best: 72.55%
Epoch 19 Val Acc: 69.95% | Best: 72.55%
Epoch 20 Val Acc: 70.50% | Best: 72.55%

üéØ ËÆ≠ÁªÉÁªìÊùüÔºåÂºÄÂßãÊúÄÁªàÊµãËØï...
üåü Final XManifold Ultra Test Acc: 75.23%
